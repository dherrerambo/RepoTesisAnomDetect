{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envia anomalias a snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snowflake-snowpark-python\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pyperclip\n",
    "start_i = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from datatools.helpers.Utils import run_time\n",
    "\n",
    "from datatools.snowflake.Querys import run_sql_query\n",
    "from datatools.snowflake.Conn import make_session\n",
    "from datatools.helpers.Utils import run_time\n",
    "from snowflake.snowpark import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols_anom(DF, models):\n",
    "    cols_anom = [sorted([c for c in DF.columns if c.lower().startswith(m)]) for m in models]\n",
    "    cols_anom = [b for a in cols_anom for b in a]\n",
    "    cols_anom = dict(DF.select(cols_anom).dtypes)\n",
    "    cols_anom = dict((k,\"number\" if v==\"bigint\" else v) for k,v in cols_anom.items())\n",
    "    cols_anom.update({\"ENSAMBLE_MAX__PROB\":\"double\", \"ENSAMBLE_MAX__POS\":\"integer\", \"PRESTADOR_PROCESADO\": \"boolean\"})\n",
    "    return cols_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols_for_dataset(snow_database, snow_schema, table, cols_anom, mode:str=\"make\", debug:bool=False):\n",
    "    for k,v in cols_anom.items():\n",
    "        if mode.lower()==\"drop\":\n",
    "            sql_columns = f\"alter table {snow_database}.{snow_schema}.{table} drop column {k};\"\n",
    "        else:\n",
    "            sql_columns = f\"alter table {snow_database}.{snow_schema}.{table} add column {k} {v};\"\n",
    "        try:\n",
    "            run_sql_query(sql_columns)\n",
    "            if debug==True: print(sql_columns)\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"{k=}\\nERROR={e}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anom_data(prestador, snow_database, snow_schema, debug:bool=False):\n",
    "    prestador_table = \"t__\"+prestador\n",
    "    if debug==True: print(f\"{prestador_table=}\\n\")\n",
    "    fName = f\"./output/{prestador}/df.parquet\"\n",
    "    df = pd.read_parquet(fName)\n",
    "    if debug==True: print(f\"{df.shape=}\")\n",
    "    \n",
    "    with make_session() as session:\n",
    "        try:\n",
    "            print(\"Tabla borrada: \", session.sql(f\"drop table {snow_database}.{snow_schema}.{prestador_table}\").collect())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        DF = session.write_pandas(\n",
    "                df=df.reset_index()\n",
    "                , database=snow_database\n",
    "                , schema=snow_schema\n",
    "                , table_name=prestador_table\n",
    "                , quote_identifiers=False\n",
    "                , auto_create_table=True\n",
    "            )\n",
    "        print(f\"Tabla de prestador dispuesta en = [{snow_database}.{snow_schema}.{prestador_table}]\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset_with_results(prestador, snow_database, snow_schema, cols_anom):\n",
    "    prestador_table = \"t__\"+prestador\n",
    "    cols_update_str = \"\\n\\t,\".join([f\"a.{c}=b.{c}\" for c in cols_anom if c not in [\"PRESTADOR_PROCESADO\"]])\n",
    "    sql_update = f\"\"\"update {snow_database}.{snow_schema}.DATASET a\\nset\\n\\t{cols_update_str}\\nfrom (\n",
    "        SELECT *, ROW_NUMBER() OVER(ORDER BY ENSAMBLE_MAX__PROB DESC, ENSAMBLE__POS) AS ENSAMBLE_MAX__POS\n",
    "        FROM (\n",
    "            SELECT *, CAST(ARRAY_MAX([LOF__PROB, IFOREST__PROB, AUTOENCODER__PROB]) AS double) AS ENSAMBLE_MAX__PROB\n",
    "            from {snow_database}.{snow_schema}.{prestador_table}\n",
    "        )\n",
    "    ) b\\nwhere a.id=b.id\"\"\"\n",
    "    sql_update = f\"\"\"-- Actualiza 'DATASET' con la info de {prestador_table}\\n\\n{sql_update}\"\"\"\n",
    "    try:\n",
    "        pyperclip.copy(sql_update)\n",
    "    except:\n",
    "        pass\n",
    "    run_sql_query(sql_update)\n",
    "    print(f\"DATASET actualizado con info de {prestador_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3F89EB097CB8FB95E71536FCE8F04D40',\n",
       " '43246AB2ECE09E419D3C7D7C1D33C33B',\n",
       " '4F09BB7381F2DD5BE74AEB854852AEFD',\n",
       " '5833AB66972CC785431A16BE6CCD2C47',\n",
       " 'B725F4EAFE01B71BF7125AB91BC8356B',\n",
       " 'BDC9655DB70D0AD154FA9811931FC6C8',\n",
       " 'E0B158A4D09C4884A37BAA919DC7EB47']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_database   = \"vp_informacion\"\n",
    "snow_schema     = \"outliers\"\n",
    "\n",
    "models = [\"lof\",\"iforest\",\"autoencoder\",\"ensamble\"]\n",
    "\n",
    "cols_anom = {'LOF': 'number',\n",
    "            'LOF__POS': 'number',\n",
    "            'LOF__PROB': 'double',\n",
    "            'LOF__SCORE': 'double',\n",
    "            'IFOREST': 'number',\n",
    "            'IFOREST__POS': 'number',\n",
    "            'IFOREST__PROB': 'double',\n",
    "            'IFOREST__SCORE': 'double',\n",
    "            'AUTOENCODER': 'number',\n",
    "            'AUTOENCODER__POS': 'number',\n",
    "            'AUTOENCODER__PROB': 'double',\n",
    "            'AUTOENCODER__SCORE': 'double',\n",
    "            'ENSAMBLE': 'number',\n",
    "            'ENSAMBLE_P_0_3': 'number',\n",
    "            'ENSAMBLE__POS': 'number',\n",
    "            'ENSAMBLE__PROB': 'double',\n",
    "            'ENSAMBLE__PROB_STD': 'double',\n",
    "            'ENSAMBLE_MAX__PROB': 'double',\n",
    "            'ENSAMBLE_MAX__POS': 'integer',\n",
    "            'PRESTADOR_PROCESADO': 'boolean'\n",
    "        }\n",
    "\n",
    "\n",
    "prestadores = ['0DD90F985DE9C9863A8073BB6BEDBED0',\n",
    "                        '3F89EB097CB8FB95E71536FCE8F04D40',\n",
    "                        '43246AB2ECE09E419D3C7D7C1D33C33B',\n",
    "                        '4F09BB7381F2DD5BE74AEB854852AEFD',\n",
    "                        '5833AB66972CC785431A16BE6CCD2C47',\n",
    "                        'B725F4EAFE01B71BF7125AB91BC8356B',\n",
    "                        'BDC9655DB70D0AD154FA9811931FC6C8',\n",
    "                        'E0B158A4D09C4884A37BAA919DC7EB47']\n",
    "prestadores = prestadores[1:]\n",
    "prestadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los campos de cols_anom ya existen.\n"
     ]
    }
   ],
   "source": [
    "## columnas en DATASET\n",
    "with make_session() as session:\n",
    "    cols_in_dataset = pd.DataFrame(session.sql(f\"show columns in table {snow_database}.{snow_schema}.DATASET\").collect())\n",
    "if len(set(cols_anom) - set(cols_in_dataset[\"column_name\"]))==0: print(\"Los campos de cols_anom ya existen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Campos anom eliminados--\n",
      "alter table vp_informacion.outliers.DATASET add column LOF number;\n",
      "alter table vp_informacion.outliers.DATASET add column LOF__POS number;\n",
      "alter table vp_informacion.outliers.DATASET add column LOF__PROB double;\n",
      "alter table vp_informacion.outliers.DATASET add column LOF__SCORE double;\n",
      "alter table vp_informacion.outliers.DATASET add column IFOREST number;\n",
      "alter table vp_informacion.outliers.DATASET add column IFOREST__POS number;\n",
      "alter table vp_informacion.outliers.DATASET add column IFOREST__PROB double;\n",
      "alter table vp_informacion.outliers.DATASET add column IFOREST__SCORE double;\n",
      "alter table vp_informacion.outliers.DATASET add column AUTOENCODER number;\n",
      "alter table vp_informacion.outliers.DATASET add column AUTOENCODER__POS number;\n",
      "alter table vp_informacion.outliers.DATASET add column AUTOENCODER__PROB double;\n",
      "alter table vp_informacion.outliers.DATASET add column AUTOENCODER__SCORE double;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE number;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE_P_0_3 number;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE__POS number;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE__PROB double;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE__PROB_STD double;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE_MAX__PROB double;\n",
      "alter table vp_informacion.outliers.DATASET add column ENSAMBLE_MAX__POS integer;\n",
      "alter table vp_informacion.outliers.DATASET add column PRESTADOR_PROCESADO boolean;\n"
     ]
    }
   ],
   "source": [
    "if len(set(cols_anom) - set(cols_in_dataset[\"column_name\"]))==0:\n",
    "    make_cols_for_dataset(snow_database=snow_database, snow_schema=snow_schema, table=\"DATASET\", cols_anom=cols_anom, mode=\"drop\")\n",
    "    print(\"--Campos anom eliminados--\")\n",
    "    make_cols_for_dataset(snow_database=snow_database, snow_schema=snow_schema, table=\"DATASET\", cols_anom=cols_anom, debug=True)\n",
    "else:\n",
    "    make_cols_for_dataset(snow_database=snow_database, snow_schema=snow_schema, table=\"DATASET\", cols_anom=cols_anom, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento del prestador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET actualizado con info de t__3F89EB097CB8FB95E71536FCE8F04D40\n",
      "DATASET actualizado con info de t__43246AB2ECE09E419D3C7D7C1D33C33B\n",
      "DATASET actualizado con info de t__4F09BB7381F2DD5BE74AEB854852AEFD\n",
      "DATASET actualizado con info de t__5833AB66972CC785431A16BE6CCD2C47\n",
      "DATASET actualizado con info de t__B725F4EAFE01B71BF7125AB91BC8356B\n",
      "DATASET actualizado con info de t__BDC9655DB70D0AD154FA9811931FC6C8\n",
      "DATASET actualizado con info de t__E0B158A4D09C4884A37BAA919DC7EB47\n"
     ]
    }
   ],
   "source": [
    "for p in prestadores:\n",
    "    try:\n",
    "        update_dataset_with_results(prestador=p, snow_database=snow_database, snow_schema=snow_schema, cols_anom=cols_anom)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR con {p}::: {str(e)}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number of rows updated  number of multi-joined rows updated\n",
      "0                  847545                                    0\n",
      "   number of rows updated  number of multi-joined rows updated\n",
      "0                 9694147                                    0\n"
     ]
    }
   ],
   "source": [
    "sql_update_procesado =[f\"\"\"UPDATE {snow_database}.{snow_schema}.DATASET a\n",
    "\t\tSET a.PRESTADOR_PROCESADO=TRUE\n",
    "\t\tFROM (\n",
    "\t\t\t\tSELECT DISTINCT PRESTADOR\n",
    "\t\t\t\tFROM {snow_database}.{snow_schema}.DATASET\n",
    "\t\t\t\tWHERE coalesce(LOF, IFOREST, AUTOENCODER) IS NOT NULL\n",
    "\t\t\t) b\n",
    "\t\tWHERE a.PRESTADOR=b.PRESTADOR\"\"\"\n",
    "\t\t, f\"\"\"UPDATE {snow_database}.{snow_schema}.DATASET a\n",
    "SET a.PRESTADOR_PROCESADO=FALSE\n",
    "WHERE a.PRESTADOR_PROCESADO IS NULL\"\"\"]\n",
    "for sql in sql_update_procesado:\n",
    "    print(run_sql_query(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecucion total: 3.99min.\n",
      "Fecha de ejecucion: 2025-06-07 12:06:28.526406\n"
     ]
    }
   ],
   "source": [
    "run_time(start_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
